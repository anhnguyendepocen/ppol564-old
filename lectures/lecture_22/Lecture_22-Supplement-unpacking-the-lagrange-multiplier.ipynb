{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> PPOL564 | DS1: Foundations </center><h1>\n",
    "<h3><center> Lecture 22 <br><br><font color='grey'> \n",
    "Constained Optimization with Equality Constraints <br>(Supplement) </font></center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lagrange Multiplier: What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/LagrangeMultipliers2D.svg/1920px-LagrangeMultipliers2D.svg.png)\n",
    "\n",
    "**Note that the gradient vectors are the point of tangency are oriented along the same direction.**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\nabla f(x,y) = \\lambda \\nabla g(x,y) $$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The Lagrange multiplier is the proportion of the two gradients. Put differently, the gradients are proportionally equal by some scaling factor $\\lambda$, which is the lagrange multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this further. \n",
    "\n",
    "Recall $f(x,y)$ is our objective function and $g(x,y) = c$ is our equality constraint, where $c$ is some constant. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\mathcal{L}(x,y,\\lambda) = f(x,y) - \\lambda (g(x,y) - c)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\nabla \\mathcal{L}(x^*,y^*,\\lambda^*) = \\textbf{0} $$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Note that when the constraint is met, $\\lambda^* (g(x^*,y^*) - c) = 0 $\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\mathcal{L}(x^*,y^*,\\lambda^*) = f(x^*,y^*) - 0 $$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ P^* = f(x^*,y^*)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Where $P^*$ is the specific condition where a stationary point exists and the constraint is met.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now think of $c$ as a variable**. When we do, the stationary points that satisfy the first order condition will change. Thus, **our stationary points become a function of whatever we set $c$ to be**.\n",
    "\n",
    "$$ P^*(c) = f(x^*(c),y^*(c)) $$\n",
    "\n",
    "The lagrange multiplier is the derivative of the function with respect to the constraint!\n",
    "\n",
    "$$ \\frac{dP^*}{dc} = \\lambda^* $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait, what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\mathcal{L}(x,y,\\lambda) = f(x,y) - \\lambda (g(x,y) - c)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(x,y,\\lambda)}{dc} = \\lambda$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall, the points that matter most (the stationary points... where our extrema live) are all a function of $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(x^*(c),y^*(c),\\lambda^*(c),c)}{dc} = f(x^*(c),y^*(c)) - \\lambda^*(c) (g(x^*(c),y^*(c)) - c)$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is now effectively a vector-valued function! To solve calculate the derivative, we need to use the **multivariate chain rule**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "#### Sidenote: Multivariate Chain Rule\n",
    "\n",
    "\n",
    "$$ \\textbf{s}(f_1(z),f_2(z)) = \\begin{pmatrix} f_1(z) \\\\ f_2(z) \\end{pmatrix}$$\n",
    "\n",
    "What is the resulting change in $\\textbf{s}( \\cdot )$ when we nudge $z$ slightly?\n",
    "\n",
    "<br>\n",
    "\n",
    "$$  \\frac{d\\textbf{s}(f_1(z),f_2(z))}{dz} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\lim_{h\\to0}\\frac{\\textbf{s}(f_1(z-h),f_2(z)) - \\textbf{s}(f_1(z),f_2(z))}{h} + \\lim_{h\\to0}\\frac{\\textbf{s}(f_1(z),f_2(z-h)) - \\textbf{s}(f_1(z),f_2(z))}{h} $$ \n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\frac{\\partial s}{\\partial f_1 } \\frac{\\partial f_1}{\\partial z} + \\frac{\\partial s}{\\partial f_2 } \\frac{\\partial f_2}{\\partial z} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "Much like the univariate chain rule, we are \"nudging through functions\". What we care about is how a nudge in the input ultimately impacts the resulting output. This logic holds in multiple dimensions: we are nudging through functions in multiple dimensions. We nudge in each dimension independently (i.e. each unit vector) and then take the combination of all our nudges.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(x^*(c),y^*(c),\\lambda^*(c),c)}{dc} = \\frac{d\\mathcal{L}}{dx^*}\\frac{dx^*}{dc} + \\frac{d\\mathcal{L}}{dy^*}\\frac{dy^*}{dc} + \\frac{d\\mathcal{L}}{d\\lambda^*}\\frac{d\\lambda^*}{dc} +  \\frac{d\\mathcal{L}}{dc}\\frac{dc}{dc}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Recall that the stationary points by definition only holds when the first derivative is zero, so we can do this...\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ 0 + 0 + 0 +  \\frac{d\\mathcal{L}}{dc}\\frac{dc}{dc} $$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "and $\\frac{dc}{dc} = 1$ so \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$ \\frac{d\\mathcal{L}(x^*(c),y^*(c),\\lambda^*(c),c)}{dc} =\\frac{d\\mathcal{L}}{dc}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "And we know from above that \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\\frac{d\\mathcal{L}}{dc} = \\lambda $$\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this tell us?**\n",
    "\n",
    "The Lagrange multiplier tells us the rate at which increases/decreases in the constraint parameter increases/decreases whatever we're trying to maximize/minimize. It is the constraint's marginal cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
